{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML4SCI/ML4SCIHackathon/blob/main/ParticleImagesChallenge/ParticleImages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3DtjEbf8M4I"
      },
      "source": [
        "# Hackathon - Particle Images\n",
        "## Problem Statement \n",
        "*   One of the important aspects of searches for new physics at the Large Hadron Collider (LHC) involves the classification of various High-Energy Particles in collision events\n",
        "*   The goal of this challenge is to develop a model which classifies electron and photon electromagnetic showers as accurately as possible based on the detector images provided in the dataset below (one pixel = one channel of the detector)\n",
        "*   The preferred metric for evaluating the model is ROC curve (Receiver Operating Characteristic curve) and the AUC (Area Under the ROC Curve) score.\n",
        "*   Although we are using Keras Framework in this sample notebook, you are free to choose Machine Learning / Deep Learning Framework of your choice. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0b6SRpWl2Xh"
      },
      "source": [
        "## Create the appropriate project folder "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYRULwKaw_A6"
      },
      "outputs": [],
      "source": [
        "!mkdir Particle_Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "at-4-Xub8DYW",
        "outputId": "7a5cc98d-018e-4104-d46b-77836d60c1bb"
      },
      "outputs": [],
      "source": [
        "!cd Particle_Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQGKM10j2CiQ"
      },
      "outputs": [],
      "source": [
        "!mkdir data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEtRyfNv9XVn"
      },
      "source": [
        "# Download the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "rcK1wY4Qt_7Y",
        "outputId": "fb09ea90-b580-4401-8c98-8fd19533bbbb"
      },
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "!wget https://cernbox.cern.ch/index.php/s/sHjzCNFTFxutYCj/download -O data/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\n",
        "!wget https://cernbox.cern.ch/index.php/s/69nGEZjOy3xGxBq/download -O data/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BepRE7pn8Du7"
      },
      "source": [
        "# Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnLzC5paz0hb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "import h5py\n",
        "from keras.layers import MaxPool2D, Input, Dense, Dropout, Flatten, Conv2D, AveragePooling2D, concatenate\n",
        "from keras.models import Model,load_model\n",
        "from keras.initializers import Constant\n",
        "from keras.callbacks import ReduceLROnPlateau,EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import AUC\n",
        "from keras.utils import to_categorical\n",
        "import pickle\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az_MoJwZ8K6l"
      },
      "source": [
        "# Keras Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzF8AHl_4yUA"
      },
      "outputs": [],
      "source": [
        "lr_init     = 1.e-3    # Initial learning rate  \n",
        "batch_size  = 500       # Training batch size\n",
        "train_size  = 100000     # Training size\n",
        "valid_size  = 50000     # Validation size\n",
        "test_size   = 5000     # Test size\n",
        "epochs      = 100       # Number of epochs\n",
        "doGPU       = False    # Use GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jX_l-WmplJx"
      },
      "source": [
        "## It is recommended to use GPU for training and inference if possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVq1XGr640nJ"
      },
      "outputs": [],
      "source": [
        "if doGPU:\n",
        "    import tensorflow.compat.v1 as tf\n",
        "    from tensorflow.compat.v1.keras.backend import set_session\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth=True\n",
        "    set_session(tf.Session(config=config))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwc56kXJ8TLo"
      },
      "source": [
        "# Load Image Data\n",
        "### Two classes of particles: electrons and photons\n",
        "### 32x32 matrices (two channels - hit energy and time) for the two classes of particles electrons and photons impinging on a calorimeter (one calorimetric cell = one pixel).\n",
        "#### Please note that although timing channel is provided, it may not necessarily help the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kr4QIMlt424u"
      },
      "outputs": [],
      "source": [
        "img_rows, img_cols, nb_channels = 32, 32, 2        \n",
        "input_dir = 'data'\n",
        "decays = ['SinglePhotonPt50_IMGCROPS_n249k_RHv1', 'SingleElectronPt50_IMGCROPS_n249k_RHv1']\n",
        "\n",
        "def load_data(decays, start, stop):\n",
        "    dsets = [h5py.File('%s/%s.hdf5'%(input_dir,decay)) for decay in decays]\n",
        "    X = np.concatenate([dset['/X'][start:stop] for dset in dsets])\n",
        "    y = np.concatenate([dset['/y'][start:stop] for dset in dsets])\n",
        "    assert len(X) == len(y)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JpHCOf38fDL"
      },
      "source": [
        "# Configure Training / Validation / Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "-RTXS58x46Fq",
        "outputId": "3a440b9a-71ad-441b-cf04-29b7a27966d9"
      },
      "outputs": [],
      "source": [
        "# Set range of training set\n",
        "train_start, train_stop = 0, train_size\n",
        "assert train_stop > train_start\n",
        "assert (len(decays)*train_size) % batch_size == 0\n",
        "X_train, y_train = load_data(decays,train_start,train_stop)\n",
        "\n",
        "# Set range of validation set\n",
        "valid_start, valid_stop = 160000, 160000+valid_size\n",
        "assert valid_stop  >  valid_start\n",
        "assert valid_start >= train_stop\n",
        "X_valid, y_valid = load_data(decays,valid_start,valid_stop)\n",
        "\n",
        "# Set range of test set\n",
        "test_start, test_stop = 204800, 204800+test_size\n",
        "assert test_stop  >  test_start\n",
        "assert test_start >= valid_stop\n",
        "X_test, y_test = load_data(decays,test_start,test_stop)\n",
        "\n",
        "samples_requested = len(decays) * (train_size + valid_size + test_size)\n",
        "samples_available = len(y_train) + len(y_valid) + len(y_test)\n",
        "assert samples_requested == samples_available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### We only use the energy channel, and we vectorize the target labels to be able to use a softmax output activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = X_train[:,:,:,0]\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "X_valid = X_train[:,:,:,0]\n",
        "y_valid = to_categorical(y_valid)\n",
        "\n",
        "X_test = X_test[:,:,:,0]\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke_NQLiz83jZ"
      },
      "source": [
        "# Define BearNet Model\n",
        "## Based on GoogleNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "OlIJuxXG7KDk",
        "outputId": "3ecb4353-6bd2-44dc-9250-aaff64169316"
      },
      "outputs": [],
      "source": [
        "\n",
        "kernel_init = 'TruncatedNormal'\n",
        "bias_init = Constant(value=5e-4)\n",
        "\n",
        "\n",
        "def inception_module(x,\n",
        "                     filters_1x1,\n",
        "                     filters_3x3_reduce,\n",
        "                     filters_3x3,\n",
        "                     filters_5x5_reduce,\n",
        "                     filters_5x5,\n",
        "                     filters_pool_proj,\n",
        "                     name=None):\n",
        "\n",
        "    conv_1x1 = Conv2D(filters_1x1, (1, 1), padding='same', activation='relu',\n",
        "                      kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
        "\n",
        "    conv_3x3 = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu',\n",
        "                      kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
        "    conv_3x3 = Conv2D(filters_3x3, (3, 3), padding='same', activation='relu',\n",
        "                      kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_3x3)\n",
        "\n",
        "    conv_5x5 = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu',\n",
        "                      kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
        "    conv_5x5 = Conv2D(filters_5x5, (5, 5), padding='same', activation='relu',\n",
        "                      kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_5x5)\n",
        "\n",
        "    pool_proj = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n",
        "    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu',\n",
        "                       kernel_initializer=kernel_init, bias_initializer=bias_init)(pool_proj)\n",
        "\n",
        "    output = concatenate([conv_1x1, conv_3x3, conv_5x5,\n",
        "                         pool_proj], axis=3, name=name)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def bearnet_truncated(input):\n",
        "    x = Conv2D(32, (3, 3), padding='same', strides=1,\n",
        "               activation='relu', name='conv_1_3x3')(input)\n",
        "    x = MaxPool2D((2, 2), padding='same', strides=2, name='max_pool_1_3x3')(x)\n",
        "    x = Conv2D(32, (2, 2), padding='same', strides=(1, 1),\n",
        "               activation='relu', name='conv_2a_2x2')(x)\n",
        "    x = Conv2D(96, (2, 2), padding='same', strides=(1, 1),\n",
        "               activation='relu', name='conv_2b_2x2')(x)\n",
        "    x = MaxPool2D((2, 2), padding='same', strides=1,\n",
        "                  name='max_pool_2_2x2/2')(x)\n",
        "\n",
        "    x = inception_module(x,\n",
        "                         filters_1x1=32,\n",
        "                         filters_3x3_reduce=48,\n",
        "                         filters_3x3=64,\n",
        "                         filters_5x5_reduce=16,\n",
        "                         filters_5x5=16,\n",
        "                         filters_pool_proj=16,\n",
        "                         name='inception_3a')\n",
        "    x = inception_module(x,\n",
        "                         filters_1x1=64,\n",
        "                         filters_3x3_reduce=64,\n",
        "                         filters_3x3=96,\n",
        "                         filters_5x5_reduce=16,\n",
        "                         filters_5x5=48,\n",
        "                         filters_pool_proj=32,\n",
        "                         name='inception_3b')\n",
        "\n",
        "    x = MaxPool2D((2, 2), padding='same', strides=(\n",
        "        2, 2), name='max_pool_3_2x2/2')(x)\n",
        "    x = inception_module(x,\n",
        "                         filters_1x1=96,\n",
        "                         filters_3x3_reduce=48,\n",
        "                         filters_3x3=104,\n",
        "                         filters_5x5_reduce=16,\n",
        "                         filters_5x5=24,\n",
        "                         filters_pool_proj=32,\n",
        "                         name='inception_4a')\n",
        "\n",
        "    x1 = AveragePooling2D((2, 2), strides=2)(x)\n",
        "    x1 = Conv2D(128, (1, 1), padding='same', activation='relu')(x1)\n",
        "    x1 = Flatten()(x1)\n",
        "    x1 = Dense(1024, activation='relu')(x1)\n",
        "    x1 = Dropout(0.4)(x1)\n",
        "    x1 = Dense(2, activation='softmax', name='output')(x1)\n",
        "    model = Model(input, [x1], name='inception_v1')\n",
        "    return model\n",
        "\n",
        "\n",
        "input = Input(shape=(32, 32, 1))\n",
        "model = bearnet_truncated(input)\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsQWFD0M86pA"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "ZGuyM25L7uc0",
        "outputId": "5e81bb7d-8edb-41ce-a0f5-f4596de7c588"
      },
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.2, patience=2, min_lr=1.e-6)\n",
        "earlystop = EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(\n",
        "    learning_rate=lr_init), metrics=['accuracy', AUC()])\n",
        "\n",
        "to_fit = False\n",
        "\n",
        "if to_fit:\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(X_valid, y_valid),\n",
        "        callbacks=[reduce_lr, earlystop],\n",
        "        verbose=1, shuffle=True\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modeldir = 'bearnet-truncated'\n",
        "model = load_model(modeldir)\n",
        "with open(f'{modeldir}/history.pkl','rb') as f_history: history = pickle.load(f_history)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLoN43278_-j"
      },
      "source": [
        "## Evaluate the Model  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "ekuQLYas7xh5",
        "outputId": "2ff140d6-493c-4dd3-8891-2213a5ee11a4"
      },
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "score = model.evaluate(X_valid, y_valid, verbose=1)\n",
        "print('\\nValidation loss / accuracy: %0.4f / %0.4f'%(score[0], score[1]))\n",
        "y_pred = model.predict(X_valid)\n",
        "fpr, tpr, _ = roc_curve(y_valid, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('Validation ROC AUC:', roc_auc)\n",
        "\n",
        "# Evaluate on test set\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('\\nTest loss / accuracy: %0.4f / %0.4f'%(score[0], score[1]))\n",
        "y_pred = model.predict(X_test)\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('Test ROC AUC:', roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "kmSRYI0R72Wn",
        "outputId": "f05db153-6b89-4c83-b654-124c5af9a7af"
      },
      "outputs": [],
      "source": [
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "#plt.legend(loc=2, prop={'size': 15})\n",
        "plt.plot(fpr, tpr, label='Model 1 (ROC-AUC = {:.3f})'.format(roc_auc))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nna7GkM45Y5w"
      },
      "source": [
        "# Submission format: \n",
        "### Please submit the Google Colab Jupyter Notebook demonstrating your solution in the similar format as illustrated in this notebook. It should contain :\n",
        "*   The final model architecture, parameters and hyper-parameters yielding the best possible performance,\n",
        "*   Its Training and Validation accuracy, \n",
        "*   ROC curve and the AUC score as shown above.\n",
        "*   Also, please submit the final trained model containing the model architecture and its trained weights along with this notebook (For example: HDF5 file, .pb file, .pt file, etc.). You are free to choose Machine Learning Framework of your choice. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek82eKnpBuI8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMgj4LNNcngXD85akp/0G7/",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "1UjgXgaYon3KKEPWnOXp5BUZgdsA2re9p",
      "name": "ParticleImages.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
